name: NLP Event Extraction Pipeline

on:
  schedule:
    # Run every 6 hours at 00:00, 06:00, 12:00, 18:00 UTC
    - cron: '0 */6 * * *'
  workflow_dispatch:
    inputs:
      hours_back:
        description: 'Number of hours to look back for articles'
        required: false
        default: '6'
        type: string
      environment:
        description: 'Environment to run in'
        required: false
        default: 'production'
        type: choice
        options:
        - development
        - staging
        - production

env:
  GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
  PYTHON_VERSION: '3.11'
  # Critical: Set UTF-8 encoding for Python
  PYTHONIOENCODING: utf-8
  LANG: en_US.UTF-8
  LC_ALL: en_US.UTF-8

jobs:
  extract-events:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Install system dependencies
      run: |
        # Install locale support
        sudo apt-get update
        sudo apt-get install -y locales
        sudo locale-gen en_US.UTF-8
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r backend/requirements.txt
        pip install groq  # Add Groq package
        
    - name: Create data directories
      run: |
        mkdir -p data/prod_events data/staging_events data/dev_events
        mkdir -p backend/data
        
    - name: Verify encoding setup
      run: |
        echo "Python version:"
        python --version
        echo "Python encoding:"
        python -c "import sys; print(f'stdout: {sys.stdout.encoding}'); print(f'default: {sys.getdefaultencoding()}')"
        echo "Locale:"
        locale
        
    - name: Download geospatial data
      run: |
        # Download Nigeria LGA data (placeholder - would use actual data source)
        curl -o backend/data/nigeria_lgas.csv https://example.com/nigeria-lgas.csv || echo "LGA data placeholder"
        curl -o backend/data/nigeria_villages.csv https://example.com/nigeria-villages.csv || echo "Village data placeholder"
        
    - name: Run NLP Pipeline
      id: extract
      env:
        HOURS_BACK: ${{ github.event.inputs.hours_back || '6' }}
        ENVIRONMENT: ${{ github.event.inputs.environment || 'production' }}
      run: |
        cd backend
        
        # Check if API key is set
        if [ -z "$GROQ_API_KEY" ]; then
          echo "ERROR: GROQ_API_KEY is not set in GitHub secrets!"
          echo "Please add it to: https://github.com/Ejyke90/naija-conflict-tracker/settings/secrets/actions"
          exit 1
        fi
        
        # Run pipeline with error handling
        python -c "
        import sys
        import os
        sys.path.append('.')
        
        print('Starting NLP Pipeline...')
        print(f'Environment: $ENVIRONMENT')
        print(f'Hours back: $HOURS_BACK')
        print(f'API Key present: {bool(os.getenv(\"GROQ_API_KEY\"))}')
        
        try:
          from app.nlp.pipeline import NLPEventExtractionPipeline, get_pipeline_config
          import json
          
          # Get configuration
          config = get_pipeline_config('$ENVIRONMENT')
          config['max_articles'] = 20  # Conservative for GitHub Actions
          
          # Initialize and run pipeline
          pipeline = NLPEventExtractionPipeline(config)
          results = pipeline.run_pipeline(hours_back=int('$HOURS_BACK'))
          
          # Print results
          print('=== NLP EXTRACTION RESULTS ===')
          print(json.dumps(results, indent=2, default=str))
          
          # Save results summary - always create the file
          with open('pipeline_results.json', 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
          # Exit with error if pipeline failed
          if results.get('status') == 'failed':
            print('Pipeline failed!')
            error_msg = results.get('error', 'Unknown error')
            print(f'Error: {error_msg}')
            sys.exit(1)
            
        except Exception as e:
          print(f'ERROR: {str(e)}')
          import traceback
          traceback.print_exc()
          
          # Create minimal results file even on failure
          error_results = {
            "status": "failed",
            "error": str(e),
            "stats": {
              "articles_scraped": 0,
              "events_extracted": 0,
              "events_verified": 0,
              "auto_published": 0,
              "pending_verification": 0,
              "rejected": 0
            }
          }
          
          with open('pipeline_results.json', 'w') as f:
            json.dump(error_results, f, indent=2)
          
          sys.exit(1)
        "
        
    - name: Check for failures
      if: failure()
      run: |
        echo "âŒ Pipeline failed. Check logs above for details."
        echo "Common issues:"
        echo "  1. 403 Forbidden: Website blocking requests (check User-Agent headers)"
        echo "  2. 400 Bad Request: Invalid Groq model (verify model name)"
        echo "  3. Encoding errors: Check PYTHONIOENCODING is set to utf-8"
        echo "  4. Missing API key: Verify GROQ_API_KEY secret is set"
        
    - name: Upload results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: extraction-results-${{ github.run_number }}
        path: |
          backend/pipeline_results.json
          backend/data/prod_events/
          backend/data/staging_events/
          backend/data/dev_events/
        retention-days: 30
        
    - name: Send to database (production only)
      if: github.event.inputs.environment == 'production' || github.event.inputs.environment == ''
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        RAILWAY_TOKEN: ${{ secrets.RAILWAY_TOKEN }}
      run: |
        cd backend
        python -c "
        import sys
        sys.path.append('.')
        import json
        import os
        from datetime import datetime
        
        # Load pipeline results
        with open('pipeline_results.json', 'r') as f:
            results = json.load(f)
        
        if results.get('status') == 'completed':
            print(f'Pipeline completed successfully')
            print(f'Events extracted: {results[\"stats\"][\"events_extracted\"]}')
            print(f'Auto-published: {results[\"stats\"][\"auto_published\"]}')
            print(f'Pending verification: {results[\"stats\"][\"pending_verification\"]}')
            
            # Here you would add code to send events to your database
            # For example, using Railway API or direct database connection
            print('Events ready for database insertion')
            
        else:
            print('Pipeline did not complete successfully')
            sys.exit(1)
        "
        
    - name: Notify Slack (optional)
      if: success() && env.SLACK_WEBHOOK_URL != ''
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
      run: |
        if [ -n "$SLACK_WEBHOOK_URL" ]; then
          curl -X POST -H 'Content-type: application/json' \
            --data '{"text":" NLP Event Extraction completed successfully!\nRun: ${{ github.run_number }}\nArticles scraped: ${{ steps.extract.outputs.articles_scraped || 0 }}\nEvents extracted: ${{ steps.extract.outputs.events_extracted || 0 }}"}' \
            $SLACK_WEBHOOK_URL
        fi

  # Optional: Quality check job
  quality-check:
    runs-on: ubuntu-latest
    needs: extract-events
    if: needs.extract-events.result == 'success'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Download results
      uses: actions/download-artifact@v4
      with:
        name: extraction-results-${{ github.run_number }}
        path: results/
        
    - name: Analyze extraction quality
      run: |
        cd results
        if [ -f "backend/pipeline_results.json" ]; then
          echo "=== EXTRACTION QUALITY ANALYSIS ==="
          python3 -c "
          import json
          
          with open('backend/pipeline_results.json', 'r') as f:
              results = json.load(f)
          
          stats = results.get('stats', {})
          
          print(f'Articles scraped: {stats.get(\"articles_scraped\", 0)}')
          print(f'Events extracted: {stats.get(\"events_extracted\", 0)}')
          print(f'Events verified: {stats.get(\"events_verified\", 0)}')
          print(f'Auto-published: {stats.get(\"auto_published\", 0)}')
          print(f'Pending verification: {stats.get(\"pending_verification\", 0)}')
          print(f'Rejected: {stats.get(\"rejected\", 0)}')
          
          # Calculate quality metrics
          if stats.get('events_extracted', 0) > 0:
              extraction_rate = (stats.get('events_extracted', 0) / stats.get('articles_scraped', 1)) * 100
              verification_rate = (stats.get('events_verified', 0) / stats.get('events_extracted', 1)) * 100
              auto_publish_rate = (stats.get('auto_published', 0) / stats.get('events_verified', 1)) * 100
              
              print(f'\\nQuality Metrics:')
              print(f'Extraction rate: {extraction_rate:.1f}%')
              print(f'Verification rate: {verification_rate:.1f}%')
              print(f'Auto-publish rate: {auto_publish_rate:.1f}%')
          "
        else
          echo "No results file found"
          exit 1
        fi
        
    - name: Create quality report
      run: |
        mkdir -p reports
        cat > reports/quality_report_${{ github.run_number }}.md << EOF
        # NLP Extraction Quality Report
        
        **Run:** ${{ github.run_number }}
        **Timestamp:** ${{ github.event.head_commit.timestamp }}
        **Environment:** ${{ github.event.inputs.environment || 'production' }}
        
        ## Summary
        - Articles scraped: $(jq '.stats.articles_scraped' results/backend/pipeline_results.json)
        - Events extracted: $(jq '.stats.events_extracted' results/backend/pipeline_results.json)
        - Events verified: $(jq '.stats.events_verified' results/backend/pipeline_results.json)
        - Auto-published: $(jq '.stats.auto_publish' results/backend/pipeline_results.json)
        - Pending verification: $(jq '.stats.pending_verification' results/backend/pipeline_results.json)
        
        ## Quality Metrics
        - Success rate: $(jq '.success_rate' results/backend/pipeline_results.json)%
        - Auto-publish rate: $(jq '.auto_publish_rate' results/backend/pipeline_results.json)%
        
        EOF
        
    - name: Upload quality report
      uses: actions/upload-artifact@v4
      with:
        name: quality-report-${{ github.run_number }}
        path: reports/
        retention-days: 90

  # Cleanup old artifacts
  cleanup:
    runs-on: ubuntu-latest
    needs: [extract-events, quality-check]
    if: always()
    
    steps:
    - name: Delete old artifacts
      uses: actions/github-script@v6
      with:
        script: |
          const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
            owner: context.repo.owner,
            repo: context.repo.repo,
            run_id: context.runId,
          });
          
          // Delete artifacts older than 7 days
          const sevenDaysAgo = new Date();
          sevenDaysAgo.setDate(sevenDaysAgo.getDate() - 7);
          
          for (const artifact of artifacts.data.artifacts) {
            const created = new Date(artifact.created_at);
            if (created < sevenDaysAgo) {
              await github.rest.actions.deleteArtifact({
                owner: context.repo.owner,
                repo: context.repo.repo,
                artifact_id: artifact.id,
              });
              console.log(`Deleted old artifact: ${artifact.name}`);
            }
          }
